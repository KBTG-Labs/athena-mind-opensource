############################################
# Logging Configuration
############################################

# The level of logging detail (e.g., INFO, DEBUG, ERROR).
LOG_LEVEL=INFO

# The level of logging detail in message queue (e.g., INFO, DEBUG, ERROR).
LOG_LEVEL_MQ=ERROR

# The timezone offset for the log timestamps.
LOG_TIMEZONE=7

############################################
# MQ Configuration
############################################

# The address of the message queue server.
MQ_BOOTSTRAP_SERVER=kafka:9092

# The group ID for the consumer group in the message queue.
MQ_CONSUMER_GROUP_ID=llm-service-group

# The minimum number of messages that need to be committed by the consumer.
MQ_CONSUMER_MIN_COMMIT_COUNT=1

# The offset reset policy for the consumer (e.g., earliest, latest).
MQ_CONSUMER_OFFSET_RESET=latest

# The maximum number of messages in a batch for the consumer.
MQ_CONSUMER_MAX_BATCH_SIZE=4

# The timeout for the consumer to consume messages.
MQ_CONSUMER_CONSUME_TIMEOUT=0.1

# The topic from which the consumer will read messages.
MQ_CONSUMER_TOPIC=llm-request

# The topic to which the producer will send responses.
MQ_PRODUCER_TOPIC=llm-response

# The topic where errors will be sent.
MQ_ERROR_TOPIC=error-queue

############################################
# Service Configuration
############################################

# The module for the language model (options: "gemini", "huggingface").
LLM_MODULE=gemini

# Flag to indicate if batch processing is required.
LLM_BATCH_REQUIRED=False

# The name of the service running the application.
SERVICE_NAME=athenamind-llm

############################################
# HuggingFace Configuration
############################################

# The API key for accessing HuggingFace.
HUGGINGFACE_API_KEY=

# The path to the HuggingFace model.
HUGGINGFACE_MODEL_PATH=

# Flag to indicate if sampling should be done when generating text.
HUGGINGFACE_MODEL_DO_SAMPLE=True

# The data type for Torch tensors (e.g., float16).
HUGGINGFACE_MODEL_TORCH_DTYPE=float16

# The maximum number of new tokens to generate.
HUGGINGFACE_MODEL_MAX_NEW_TOKENS=2048

# The device ID for Torch (e.g., GPU device ID).
HUGGINGFACE_MODEL_TORCH_DEVICE=0

############################################
# Gemini Configuration
############################################

# The API key for accessing Gemini services.
GEMINI_API_KEY=

# The name of the Gemini model to use.
GEMINI_MODEL_NAME=gemini-pro

# The maximum number of output tokens to generate.
GEMINI_MAX_OUTPUT_TOKEN=2048

# The temperature parameter for text generation, controlling randomness.
GEMINI_TEMPERATURE=0.9

# The top-p parameter for nucleus sampling in text generation.
GEMINI_TOP_P=1

# Flag to indicate if streaming is required for responses.
GEMINI_STREAM_REQUIRED=True

# The number of worker threads for handling requests.
GEMINI_REQUEST_WORKERS=4

# The transport protocol for communicating with Gemini (e.g., grpc).
GEMINI_TRANSPORT=grpc

############################################
# Opentelemetry Configuration
############################################

# Flag to indicate if Opentelemetry is required.
ENABLE_TELEMETRY=False

# The address of the Opentelemetry collector.
TELEMETRY_COLLECTOR_ENDPOINT=

